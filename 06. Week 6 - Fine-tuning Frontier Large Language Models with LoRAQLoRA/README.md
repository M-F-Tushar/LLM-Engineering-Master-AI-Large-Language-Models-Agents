# LLM Fine-Tuning Course: Complete Structured Guide

## 1. Core Transition: From Inference to Training

### Inference vs Training
- **Inference**: Using a pre-trained model to predict the next token at runtime
- **Training**: Tweaking the model's billions of parameters to make it better at predicting future tokens

### Transfer Learning
- **Concept**: Taking an existing trained model and continuing training with specialized data
- **Why needed**: Training from scratch costs $100+ million (beyond most budgets)
- **Benefit**: Transfers existing knowledge while adding specialized skills

### Fine-Tuning
- **Definition**: The process of making a pre-trained model more precisely trained for your specific task
- **Techniques**: Uses methods like LoRA (Low-Rank Adaptation) to manage memory efficiently

## 2. Project Overview: Product Price Prediction

### Business Problem
- **Goal**: Build a model that estimates product prices based only on descriptions
- **Scope**: Electronic products, home appliances, cars, etc.
- **Why this problem**: Easy to measure success (price accuracy is human-understandable)

### Why LLMs for This Task?
- **Traditional approach**: Would typically use regression models
- **LLM advantage**: Frontier models surprisingly excel at quantitative tasks
- **Measurement benefit**: Unlike translation quality, price accuracy is objectively measurable

## 3. Data Sources and Collection

### Data Source Hierarchy
1. **Proprietary company data** (first choice)
2. **Kaggle** (public datasets)
3. **Hugging Face** (community-contributed data)
4. **Synthetic data** (generated by LLMs)
5. **Specialist companies** (like Scale.ai for curated datasets)

### Amazon Reviews Dataset
- **Source**: Scraped Amazon reviews from late 2023
- **Size**: Nearly 50 million items across multiple categories
- **Content**: Product descriptions, prices, reviews, and metadata

## 4. Data Curation Process (6 Steps)

### Step 1: Investigation
- Understanding data fields and quality
- Checking how well-populated the data is
- Identifying data quality issues

### Step 2: Parsing
- Converting raw data into structured objects
- Creating an `Item` class for easier handling
- Standardizing data formats

### Step 3: Visualization
- Creating histograms of price distributions
- Understanding data spread and skewness
- Identifying outliers (like the $21,000 microwave!)

### Step 4: Data Quality Assessment
- Finding limitations in the data
- Discovering only ~50% of items have prices
- Identifying data imbalances

### Step 5: Curation
- **Price filtering**: Limiting to $0.50 - $999.49 range
- **Token limiting**: Constraining to ~180 tokens per item
- **Content cleaning**: Removing distracting elements like part numbers
- **Balancing**: Creating better representation across price ranges

### Step 6: Saving and Uploading
- Shuffling data for training
- Splitting into training (400K) and test (2K) sets
- Uploading to Hugging Face Hub

## 5. Key Technical Concepts

### Tokenization
- **Tokens**: Basic units that models process (words broken into pieces)
- **Token limits**: Constraining to 180 tokens max per training example
- **Why important**: Memory management and cost control

### Data Scrubbing Techniques
- **Part number removal**: Filtering out 8+ character codes with numbers
- **Character cleaning**: Removing weird characters and multiple spaces
- **Content filtering**: Removing distracting phrases like "batteries included"

### Prompt Engineering for Training
- **Training prompt**: Includes both description and answer price
- **Test prompt**: Only description, model must predict price
- **Format**: "How much does this cost to the nearest dollar?" followed by description

## 6. Performance Evaluation Metrics

### Model-Centric Metrics
- **Training Loss**: How well model performs on training data
- **Validation Loss**: Performance on held-back validation data
- **RMSLE**: Root Mean Squared Log Error (balances absolute and percentage errors)
- **MSE**: Mean Squared Error (problematic for large price differences)

### Business-Centric Metrics
- **Average Absolute Price Difference**: Simple "how wrong was the model?"
- **Percentage Price Difference**: Relative error measurement
- **Hit Rate**: Percentage meeting criteria (e.g., within $40 OR within 20%)

## 7. Strategy Framework (5 Steps)

### Step 1: Understanding
- Gather business requirements
- Define success metrics
- Assess data quantity, quality, and format
- Determine non-functional requirements (budget, latency, scale)

### Step 2: Preparation
- Research existing solutions
- Compare relevant LLMs
- Curate and clean data
- Split data into train/validation/test sets

### Step 3: Selection
- Choose LLMs based on criteria
- Experiment with different models
- Train and validate with curated data

### Step 4: Customization (Optimization Techniques)
Three main approaches:
- **Prompting**: Multi-shot, chaining, tools (inference-time)
- **RAG**: Injecting relevant context (inference-time)
- **Fine-tuning**: Training model weights (training-time)

### Step 5: Productionizing
- Define API for model access
- Handle hosting and deployment
- Monitor performance and security
- Measure business metrics
- Continuous improvement

## 8. When to Use Each Optimization Technique

### Prompting
- **Best for**: Quick improvements, low cost, direct results
- **Limitations**: Context window limits, diminishing returns with more context

### RAG (Retrieval Augmented Generation)
- **Best for**: High accuracy needs, existing knowledge bases, scalable data
- **Limitations**: More complex to build, lacks nuanced understanding

### Fine-Tuning
- **Best for**: Specialized tasks, high data volume, deep expertise needs
- **Limitations**: Expensive, requires lots of data, risk of "catastrophic forgetting"

## 9. Baseline Models (Traditional ML)

### Purpose of Baselines
- Provide measurement yardstick
- Validate that LLMs are actually better
- Sometimes simpler solutions work just as well

### Types Covered
1. **Random guessing**: Random number between 1-999
2. **Average guessing**: Always guess the training set average
3. **Feature engineering**: Extract meaningful features manually
4. **Bag of Words**: Count word occurrences in descriptions
5. **Word2Vec**: Neural word embeddings
6. **Linear Regression**: Linear combination of features
7. **Random Forests**: Ensemble of decision trees
8. **Support Vector Regression**: Advanced separation techniques

### Testing Framework
- Created `Tester` class for consistent model evaluation
- Tests on 250 data points
- Provides visual scatter plots (predicted vs. actual)
- Color coding: Green (good), Yellow (fair), Red (poor)

## 10. Data Distribution Insights

### Price Distribution
- **Original**: Heavily skewed toward cheap items (average $6)
- **After curation**: Better balanced but still realistic (average ~$100)
- **Price spikes**: Common at psychological price points ($399, $499)

### Category Distribution
- **Automotive**: Largest category (900K+ items)
- **Electronics**: Second largest (400K+ items)
- **Final dataset**: 2.8M items reduced to 400K for training

### Token-Price Correlation
- Weak correlation between description length and price
- Longer descriptions don't necessarily mean higher prices
- Traditional ML likely won't find strong linear relationships

---

## 11. Feature Engineering in Traditional ML (Detailed Implementation)

### Core Concepts Explained

**Feature Engineering**: The process of manually selecting and creating input variables (features) that help machine learning models make better predictions. Think of it as telling the model "these are the important things to pay attention to."

### Why Feature Engineering Matters
Before deep learning, data scientists had to be domain experts who manually identified which aspects of data would be most useful for predictions. For product pricing, this meant figuring out what makes one product more expensive than another.

### Features Implemented in the Course

#### 1. Weight Feature
**Challenge**: Weight data was extremely messy
- Some weights in pounds, others in ounces, kilograms, milligrams
- Required complex parsing logic to standardize units
- **Solution**: Convert everything to pounds as baseline unit
- **Default handling**: Items without weight got assigned average weight (13.6 pounds)

**Why weight matters**: Heavier items often cost more (correlation with size/materials)

#### 2. Best Sellers Rank Feature
**Concept**: Products ranked on Amazon's bestseller lists
**Challenge**: Products can appear on multiple lists with different ranks
**Solution**: Take average rank across all lists where product appears
**Default handling**: Items without ranks get assigned average rank (~380,000)

**Why rank matters**: Popular items might have different pricing patterns

#### 3. Text Length Feature
**Simple but effective**: Count of characters in product description
**Reasoning**: Longer descriptions might indicate more expensive, detailed products
**Implementation**: Basic `len()` function on description text

#### 4. Brand Feature (Top Electronics Brands)
**Manual curation**: Identified major electronics brands (HP, Dell, Samsung, Apple, etc.)
**Binary feature**: 1 if product is from top electronics brand, 0 otherwise
**Domain knowledge required**: This is where human expertise was crucial in traditional ML

### Feature Engineering Challenges

#### The "Grotty Work" Problem
Feature engineering is described as "distasteful, unsavory work" because it involves:
- Deep diving into messy, real-world data
- Writing lots of custom parsing logic
- Making arbitrary decisions about edge cases
- Requiring domain expertise in the business area

#### Default Value Strategy
**Problem**: Not all products have all features populated
**Solution**: Replace missing values with training set averages
- Missing weight → Average weight
- Missing rank → Average rank
- This is a common technique called "mean imputation"

#### Domain Expertise Requirement
Traditional ML required data scientists to understand the business domain deeply. For product pricing, you'd need to know:
- Which car brands are premium vs. budget
- How electronics brands compare
- What product features indicate quality

**Modern advantage**: Deep neural networks learn these patterns automatically from data, removing the need for manual feature engineering.

---

## 12. Natural Language Processing Approaches

### Bag of Words Model

**Core concept**: Treat text as a "bag" of words, ignoring order and grammar, just counting occurrences.

**How it works**:
1. Create vocabulary of most common 1,000 words
2. For each document, count how many times each word appears
3. Create vector where each position represents a word count
4. Remove "stop words" (and, the, it, etc.) that don't add meaning

**Example**: "The phone is red and the phone is new" becomes:
- phone: 2
- red: 1
- new: 1
(ignoring "the", "is", "and")

**CountVectorizer**: Library tool that automates this process

### Word2Vec Model

**Advancement over Bag of Words**: Creates dense vector representations where similar words have similar vectors.

**Key difference**: Instead of just counting words, Word2Vec understands that "phone" and "mobile" are related concepts.

**Implementation details**:
- 400-dimensional vectors (vs. 1,000 dimensions for Bag of Words)
- Uses neural networks to learn word relationships
- Takes several minutes to train on large dataset

**Surprising result**: In this case, Word2Vec performed slightly worse than simple Bag of Words, possibly because linear regression couldn't fully utilize the richer representations.

---

## 13. Advanced Machine Learning Models

### Support Vector Regression (SVR)

**Core concept**: Find the best line (hyperplane) that separates data points, focusing on the most difficult cases.

**Key features**:
- Uses "support vectors" (the hardest-to-classify points) to define decision boundary
- Can handle non-linear relationships with different "kernels"
- Linear kernel used for speed (non-linear kernels take much longer)

**Performance**: Slight improvement over simpler models but not dramatic

### Random Forest Regression

**Ensemble method**: Combines many smaller decision trees to make better predictions.

**How it works**:
1. Create many decision trees
2. Each tree uses random subset of data points
3. Each tree uses random subset of features
4. Final prediction = average of all tree predictions

**Advantages**:
- Robust to overfitting
- Handles different data types well
- Few hyperparameters to tune
- Generally good "out of the box" performance

**Result**: Best traditional ML model with $97 average error

**Why it worked well**: Could capture complex non-linear patterns in the relationship between product descriptions and prices.

---

## 14. Frontier Model Evaluation

### Human Baseline Experiment

**Methodology**: Manual price estimation of 250 products by human evaluator
**Result**: $127 average error (worse than Random Forest)
**Key insights**:
- Humans lack knowledge of specialized product categories
- Fatigue affects performance over time
- Demonstrates objective difficulty of the task
- Shows that good ML models can outperform humans on narrow tasks

### GPT Model Performance

#### GPT-4 Mini Results
**Performance**: $79.58 average error
**Key achievements**:
- Beat all traditional ML models without any training data
- Used only world knowledge from pre-training
- 52% hit rate (predictions within acceptable range)

#### GPT-4 Full Model Results
**Performance**: $76 average error
**Improvement**: Modest gain over Mini version
**Insight**: Mini and full versions have similar capabilities for this task

#### Claude 3.5 Sonnet Results
**Performance**: $97 average error (similar to Random Forest)
**Notable issue**: Made one extremely high prediction ($4,999 vs $495 actual)
**Lesson**: Different models have different strengths and weaknesses

### Test Data Contamination Concerns
**Issue**: Models might have seen Amazon product data during training
**Evidence against contamination**: No suspiciously perfect predictions observed
**Conclusion**: Results likely reflect genuine world knowledge rather than memorization

---

## 15. Fine-Tuning Frontier Models

### When to Fine-Tune Frontier Models

**OpenAI's Recommended Use Cases**:
1. **Style/Tone adjustment**: Adding sarcasm, formality, etc.
2. **Format reliability**: Ensuring consistent output structure
3. **Complex prompt following**: Handling difficult instructions
4. **Edge case handling**: Fixing occasional failures
5. **New tasks**: Hard-to-describe tasks that can't be prompted effectively

### Fine-Tuning Process

#### Data Preparation
**JSONL Format**: JSON Lines - each line is separate JSON object (not array)
```json
{"messages": [{"role": "system", "content": "..."}, {"role": "user", "content": "..."}, {"role": "assistant", "content": "..."}]}
{"messages": [{"role": "system", "content": "..."}, {"role": "user", "content": "..."}, {"role": "assistant", "content": "..."}]}
```

**Training set size**: 500 examples (more than OpenAI's 50-100 recommendation)
**Validation set**: 50 examples for monitoring

#### Training Configuration
- **Model**: GPT-4 Mini (cheaper inference, similar performance)
- **Epochs**: 1 (no repetition through data)
- **Seed**: For reproducible results
- **Integration**: Weights & Biases for monitoring

### Training Monitoring

**Weights & Biases Integration**:
- Real-time loss tracking
- Training vs validation loss comparison
- Visual progress monitoring

**Training Loss Patterns**:
- Initial dramatic drop (learning basic structure)
- Subsequent volatility (exploring different predictions)
- Expected trend: gradual decrease over time

**Concerning pattern observed**: Loss plateaued rather than consistently decreasing

### Fine-Tuning Results and Analysis

**Performance**: $91 average error (worse than pre-fine-tuning $80)
**Disappointing outcome**: Fine-tuning didn't improve the main business metric

**Potential explanations**:
1. **Catastrophic forgetting**: New training eroded existing knowledge
2. **Insufficient improvement signal**: 500 examples couldn't improve already-strong world knowledge
3. **Prompting already optimal**: Clear instructions left little room for improvement
4. **Task mismatch**: Price prediction better suited to world knowledge than fine-tuning

### Lessons Learned

**Key insight**: Frontier models with trillions of parameters and vast training data may not benefit from small amounts of additional training for knowledge-intensive tasks.

**When fine-tuning helps most**:
- Style/format changes
- Handling specific edge cases
- Tasks requiring particular response patterns

**When fine-tuning may not help**:
- Tasks requiring broad world knowledge
- Well-performing base model with clear prompts
- Quantitative tasks that benefit from pre-trained knowledge

---

## 16. Key Technical Implementation Details

### Prompt Engineering for Fine-Tuning

**Training prompt structure**:
```
System: "You estimate prices of items, reply only with the price, no explanation"
User: "How much does this cost? [product description]"
Assistant: "Price is $X"
```

**Key technique**: Partial assistant response to force price completion
- Provides "Price is $" as assistant message
- Forces model to continue with most likely next token (the price)

### Price Extraction Logic
**Robust parsing**: Extract floating-point numbers from any response format
**Handles variations**: "$99.99", "roughly $100", "The price is $50"
**Fallback strategy**: Regex patterns to find numbers in text

### Reproducibility Considerations
**OpenAI seed parameter**: Attempts to ensure consistent outputs
**Limitation**: Model updates can still cause variations
**Best practice**: Document exact model versions used

### Cost Management
**Fine-tuning cost**: ~$0.05 per run (free during promotional period)
**Inference cost**: Pennies for 250 predictions
**Optimization**: Limited max_tokens to reduce unnecessary generation

---

## 17. Advanced Concepts Explained Simply

### Epochs
**Definition**: One complete pass through all training data
**Example**: If you have 1000 training examples, 1 epoch = model sees all 1000 once
**Multiple epochs**: Same data seen multiple times (can cause overfitting)

### Hyperparameters
**Simple explanation**: Settings you can adjust to change how training works
**Examples**: Learning rate, number of epochs, model size
**Hyperparameter optimization**: Fancy term for "try different settings and see what works"

### Overfitting vs Underfitting
**Overfitting**: Model memorizes training data but fails on new data (like cramming for exam)
**Underfitting**: Model too simple to capture patterns (like not studying enough)
**Goal**: Find sweet spot where model generalizes well to new data

### Training Loss vs Validation Loss
**Training loss**: How well model performs on data it's learning from
**Validation loss**: How well model performs on data it hasn't seen
**Divergence**: If training loss decreases but validation loss increases = overfitting

### Catastrophic Forgetting
**Simple explanation**: When learning new things makes AI forget old things
**Example**: Teaching model to price cars might make it worse at pricing phones
**Challenge**: Especially relevant for fine-tuning where new data might conflict with pre-training

---

## Key Takeaways

1. **Data quality matters most**: Improving data has more impact than hyperparameter tuning
2. **Balance is crucial**: Skewed datasets hurt model performance
3. **Baselines are essential**: Always establish what you're improving upon
4. **Multiple techniques exist**: Choose based on your specific needs and constraints
5. **Measurement clarity**: Having clear, human-understandable metrics is invaluable
6. **Fine-tuning isn't always better**: Frontier models may already be optimal for many tasks
7. **Traditional ML still valuable**: Sometimes simpler approaches work well
8. **Domain expertise evolution**: Deep learning reduces need for manual feature engineering
9. **Evaluation frameworks crucial**: Consistent testing methodology enables fair comparisons
10. **Real-world complexity**: Messy data and edge cases are the norm, not exception
